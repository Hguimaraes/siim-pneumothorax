{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "os.chdir(os.path.dirname(\"../src/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siim_pneumothorax.utils import parse_dicom\n",
    "from siim_pneumothorax.utils import ConfigObject\n",
    "from siim_pneumothorax.dataset import get_pneumo_loaders\n",
    "from siim_pneumothorax.losses import Dice_metric\n",
    "from siim_pneumothorax.losses import IoU_metric\n",
    "from siim_pneumothorax.losses import MixedLoss\n",
    "from siim_pneumothorax.losses import MixedFocalLoss\n",
    "from siim_pneumothorax.losses import DiceLoss\n",
    "from siim_pneumothorax.losses import FocalLoss\n",
    "from siim_pneumothorax.models import VanillaUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change \n",
    "# CONFIG_SAVE_PATH\n",
    "# NUM_EPOCHS\n",
    "# GRID_SIZE\n",
    "# lr_scheduler\n",
    "\n",
    "params = {\n",
    "    # Loader parameters\n",
    "    'loader_params': {\n",
    "        'img_size': 256,\n",
    "        'batch_size': 8,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 8,\n",
    "        'rgb_channel': False,\n",
    "        'grid_size': 16\n",
    "    },\n",
    "    \n",
    "    # Loss/metric parameters\n",
    "    'alpha': 0.75,\n",
    "    'gamma': 2,\n",
    "    'smooth': 1,\n",
    "    \n",
    "    # training parameters\n",
    "    'n_folds': 3,\n",
    "    'num_epochs': 30,\n",
    "    'checkpoint_path': '../models/model_checkpoint',\n",
    "    'config_save_path': '../models/stage_2_config'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns = sorted(glob('../data/siim/dicom-images-train/*/*/*.dcm'))\n",
    "test_fns = sorted(glob('../data/siim/dicom-images-test/*/*/*.dcm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rle_codes = pd.read_csv('../data/siim/train-rle.csv')\n",
    "train_rle_codes.columns = [\"ImageId\", \"EncodedPixels\"]\n",
    "train_rle_codes['has_pneumothorax'] = (train_rle_codes.EncodedPixels != \"-1\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame([parse_dicom(x, train_rle_codes) for x in  train_fns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.DataFrame([parse_dicom(x, is_training=False) for x in  test_fns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.sample(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, Loss function, Optimizer and LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(params, fold):\n",
    "    device = 'cuda'\n",
    "    model = smp.Unet('resnet34', encoder_weights='imagenet', in_channels=1)\n",
    "    model = model.load_state_dict(\n",
    "        torch.load('../models/stage_1_config_fold_{}.pt'.format(fold))['model_state_dict']\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Build optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    optimizer = optimizer.load_state_dict(\n",
    "        torch.load('../models/stage_1_config_fold_{}.pt'.format(fold))['optimizer_state_dict']\n",
    "    )\n",
    "    \n",
    "    # LR scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    m_loss = MixedFocalLoss(params['smooth'])\n",
    "    \n",
    "    return device, model, optimizer, lr_scheduler, m_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_loader, valid_loader, \n",
    "          loss_fn, lr_scheduler, optimizer, device, debug=True):\n",
    "\n",
    "    history = {\n",
    "        'loss': [], 'val_loss': [], 'dice': [], \n",
    "        'val_dice': [], 'iou': [], 'val_iou': []\n",
    "    }\n",
    "    \n",
    "    dice_metric = Dice_metric()\n",
    "    iou_metric = IoU_metric()\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        loss, dice, iou = 0.0, [], []\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        for idx, (img, mask) in enumerate(train_loader):\n",
    "            img = img.type(torch.FloatTensor).to(device)\n",
    "            mask = mask.type(torch.FloatTensor).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch = model(img)\n",
    "            loss_batch = loss_fn(recon_batch, mask)\n",
    "            dice_batch = dice_metric(recon_batch, mask)\n",
    "            iou_batch = iou_metric(recon_batch, mask)\n",
    "            \n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute metrics to all batch\n",
    "            loss += loss_batch.item()*len(img)\n",
    "            dice.append(dice_batch.item())\n",
    "            iou.append(iou_batch.item())\n",
    "\n",
    "            if debug:\n",
    "                print(\"step: {:4d} of {:4d} | loss: {:.4f} | dice: {:.4f} | iou: {:.4f} \".format(idx + 1, len(train_loader),\n",
    "                    loss_batch.item()*len(img), dice_batch.item(), iou_batch.item()), end='\\r')\n",
    "\n",
    "        loss /= len(train_loader)\n",
    "        dice = np.nanmean(dice)\n",
    "        iou = np.nanmean(iou)\n",
    "\n",
    "        val_loss, val_dice, val_iou, comparison = evaluate(config, model, valid_loader, loss_fn, device)\n",
    "        lr_scheduler.step(val_loss)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Display training Metrics\n",
    "        print('====> Epoch: {:02d} Loss: {:.3f} | val_loss: {:.3f} | val_dice: {:.3f} | val_iou: {:.3f} | Elapsed time: {:.4f}'.format(\n",
    "          epoch+1, loss, val_loss, val_dice, val_iou, elapsed_time))\n",
    "        \n",
    "        # Compute the statistics of the last epoch and save to history\n",
    "        history['loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['dice'].append(dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['iou'].append(iou)\n",
    "        history['val_iou'].append(val_iou)\n",
    "\n",
    "        # Checkpoint the model\n",
    "        torch.save(model.state_dict(), config.checkpoint_path)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, model, heldout_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    heldout_loss = 0\n",
    "    heldout_dice = []\n",
    "    heldout_iou = []\n",
    "    \n",
    "    dice_metric = Dice_metric()\n",
    "    iou_metric = IoU_metric()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, mask) in enumerate(heldout_loader):\n",
    "            img = img.type(torch.FloatTensor).to(device)\n",
    "            mask = mask.type(torch.FloatTensor).to(device)\n",
    "            recon_batch = model(img)\n",
    "            heldout_loss += loss_fn(recon_batch, mask).item()*len(img)\n",
    "            heldout_dice.append(dice_metric(recon_batch, mask).item())\n",
    "            heldout_iou.append(iou_metric(recon_batch, mask).item())\n",
    "\n",
    "    heldout_loss /= len(heldout_loader)\n",
    "    heldout_dice = np.nanmean(heldout_dice)\n",
    "    heldout_iou = np.nanmean(heldout_iou)\n",
    "    \n",
    "    n = min(img.size(0), 16)\n",
    "    comparison = [img[:n], recon_batch[:n], mask[:n]]\n",
    "    \n",
    "    return heldout_loss, heldout_dice, heldout_iou, comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldTrainer(dataset, params):\n",
    "    metrics = []\n",
    "    skf = StratifiedKFold(n_splits=params['n_folds'])\n",
    "\n",
    "    # Start kFold\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(dataset, dataset['has_pneumothorax'])):\n",
    "        print(\"======= Fold {}/{} =======\".format(fold+1, params['n_folds']))\n",
    "        train_dataset, val_dataset = dataset.iloc[train_index], dataset.iloc[val_index]\n",
    "        \n",
    "        # Balance the dataset\n",
    "        g = train_dataset.groupby('has_pneumothorax')\n",
    "        train_dataset = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "        \n",
    "        # Configure parameters\n",
    "        config = {\n",
    "            'smooth': params[\"smooth\"],\n",
    "            'num_epochs': params['num_epochs'],\n",
    "            'checkpoint_path': '{}.pt'.format(params['checkpoint_path']),\n",
    "            'config_save_path': '{}_fold_{}.pt'.format(params['config_save_path'], fold)\n",
    "        }\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = get_pneumo_loaders(df=train_dataset, is_train=True, **params['loader_params'])\n",
    "        val_loader = get_pneumo_loaders(df=val_dataset, is_train=False, **params['loader_params'])\n",
    "\n",
    "        # Start training\n",
    "        config = ConfigObject(**config)\n",
    "        device, model, optimizer, lr_scheduler, m_loss = get_parameters(params, fold)\n",
    "        history = train(\n",
    "            config, model, train_loader, val_loader, \n",
    "            m_loss, lr_scheduler, optimizer, device, debug=True\n",
    "        )\n",
    "\n",
    "        metrics.append(history)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, config.config_save_path)\n",
    "        print(\"\") # Break line\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Fold 1/3 =======\n",
      "====> Epoch: 01 Loss: 7.917 | val_loss: 7.755 | val_dice: 0.042 | val_iou: 0.778 | Elapsed time: 43.0143\n",
      "====> Epoch: 02 Loss: 7.579 | val_loss: 7.768 | val_dice: 0.078 | val_iou: 0.778 | Elapsed time: 41.9989\n",
      "====> Epoch: 03 Loss: 7.410 | val_loss: 7.423 | val_dice: 0.099 | val_iou: 0.778 | Elapsed time: 42.8605\n",
      "====> Epoch: 04 Loss: 7.218 | val_loss: 7.108 | val_dice: 0.131 | val_iou: 0.778 | Elapsed time: 42.7769\n",
      "====> Epoch: 05 Loss: 7.276 | val_loss: 7.589 | val_dice: 0.095 | val_iou: 0.778 | Elapsed time: 42.4942\n",
      "====> Epoch: 06 Loss: 7.148 | val_loss: 7.456 | val_dice: 0.120 | val_iou: 0.778 | Elapsed time: 41.9750\n",
      "====> Epoch: 07 Loss: 7.102 | val_loss: 7.114 | val_dice: 0.142 | val_iou: 0.778 | Elapsed time: 42.5690\n",
      "====> Epoch: 08 Loss: 7.071 | val_loss: 7.352 | val_dice: 0.117 | val_iou: 0.778 | Elapsed time: 43.1577\n",
      "====> Epoch: 09 Loss: 6.953 | val_loss: 6.840 | val_dice: 0.170 | val_iou: 0.778 | Elapsed time: 42.6284\n",
      "====> Epoch: 10 Loss: 6.875 | val_loss: 6.689 | val_dice: 0.187 | val_iou: 0.778 | Elapsed time: 43.1005\n",
      "====> Epoch: 11 Loss: 6.892 | val_loss: 7.079 | val_dice: 0.151 | val_iou: 0.778 | Elapsed time: 43.2830\n",
      "====> Epoch: 12 Loss: 6.782 | val_loss: 6.884 | val_dice: 0.173 | val_iou: 0.778 | Elapsed time: 41.6447\n",
      "====> Epoch: 13 Loss: 6.798 | val_loss: 6.847 | val_dice: 0.169 | val_iou: 0.778 | Elapsed time: 41.8059\n",
      "====> Epoch: 14 Loss: 6.692 | val_loss: 6.795 | val_dice: 0.186 | val_iou: 0.778 | Elapsed time: 42.9926\n",
      "====> Epoch: 15 Loss: 6.714 | val_loss: 6.858 | val_dice: 0.173 | val_iou: 0.778 | Elapsed time: 42.5418\n",
      "====> Epoch: 16 Loss: 6.734 | val_loss: 6.642 | val_dice: 0.194 | val_iou: 0.778 | Elapsed time: 43.3464\n",
      "====> Epoch: 17 Loss: 6.693 | val_loss: 6.898 | val_dice: 0.174 | val_iou: 0.778 | Elapsed time: 42.6610\n",
      "====> Epoch: 18 Loss: 6.589 | val_loss: 6.530 | val_dice: 0.206 | val_iou: 0.778 | Elapsed time: 42.6112\n",
      "====> Epoch: 19 Loss: 6.692 | val_loss: 6.754 | val_dice: 0.184 | val_iou: 0.778 | Elapsed time: 43.0699\n",
      "====> Epoch: 20 Loss: 6.576 | val_loss: 6.811 | val_dice: 0.183 | val_iou: 0.778 | Elapsed time: 42.0399\n",
      "====> Epoch: 21 Loss: 6.619 | val_loss: 6.460 | val_dice: 0.217 | val_iou: 0.778 | Elapsed time: 43.3913\n",
      "====> Epoch: 22 Loss: 6.606 | val_loss: 6.679 | val_dice: 0.193 | val_iou: 0.778 | Elapsed time: 42.5671\n",
      "====> Epoch: 23 Loss: 6.548 | val_loss: 6.524 | val_dice: 0.208 | val_iou: 0.778 | Elapsed time: 44.2610\n",
      "====> Epoch: 24 Loss: 6.577 | val_loss: 6.523 | val_dice: 0.211 | val_iou: 0.778 | Elapsed time: 42.9687\n",
      "====> Epoch: 25 Loss: 6.668 | val_loss: 6.497 | val_dice: 0.212 | val_iou: 0.778 | Elapsed time: 42.9819\n",
      "====> Epoch: 26 Loss: 6.475 | val_loss: 6.519 | val_dice: 0.210 | val_iou: 0.778 | Elapsed time: 41.6927\n",
      "====> Epoch: 27 Loss: 6.552 | val_loss: 6.524 | val_dice: 0.208 | val_iou: 0.778 | Elapsed time: 42.8539\n",
      "step:  445 of  445 | loss: 4.5330 | dice: 0.3008 | iou: 0.1667 \r"
     ]
    }
   ],
   "source": [
    "history = KFoldTrainer(train_dataset, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss = mean: {:.3f} | std: {:.3f}\".format(\n",
    "    np.mean([d['val_loss'][-1] for d in history]),\n",
    "    np.std([d['val_loss'][-1] for d in history])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dice = mean: {:.3f} | std: {:.3f}\".format(\n",
    "    np.mean([d['val_dice'][-1] for d in history]),\n",
    "    np.std([d['val_dice'][-1] for d in history])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IoU = mean: {:.3f} | std: {:.3f}\".format(\n",
    "    np.mean([d['val_iou'][-1] for d in history]), \n",
    "    np.std([d['val_iou'][-1] for d in history])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet('resnet34', encoder_weights='imagenet', in_channels=1)\n",
    "model.load_state_dict(torch.load('../models/stage_1_config_fold_0.pt')['model_state_dict'])\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_pneumo_loaders(df=train_dataset, is_train=False, **params['loader_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for img, mask in train_loader:\n",
    "        img = img.to('cuda')\n",
    "        predicted_mask = model(img)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "\n",
    "p_mask = (nn.Sigmoid()(predicted_mask[idx][0, :, :]).cpu() > .5).int()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 15))\n",
    "axes[0].imshow(img[idx, 0, :, :].cpu(), cmap='gray')\n",
    "axes[1].imshow(mask[idx, 0, :, :], cmap='gray')\n",
    "axes[2].imshow(p_mask, cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pneumo_seg",
   "language": "python",
   "name": "pneumo_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
